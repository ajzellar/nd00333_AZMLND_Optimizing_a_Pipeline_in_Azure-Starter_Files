# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains bank marketing data about many different people and whether they have agreed to sign up for what ever is being marketed. We seek to predict based will say yes or no to what they are being marketed.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline architechture:

1. Get the data
    * We get the data from in csv format from the following [link](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv)
1. Cleaning the data
    * The data is cleaned using a python function which was provided in the train.py script
    * Most of the clean up consists of taking binary values and replacing them with 0 or 1
1. Splitting the data
    * The data is split into traing and test sets using the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn. I decided to use split the data 80 - 20 meaning 80% of the data is ussed for training and 20% is used for testing.
1. Hyperparameter Tuning with HyperDrive
    * For hyperparameter tuning I used hyperdrive to train the C parameter (Inverse of regularization strength) and the max_iter parameter (Maximum number of iterations to converge) I used Random Parameter Sampling for my parameter sampler (more info on that below) and a BanditPolicy for my early termination policy (more info below). I set up hyperdrive to maximize the accuracy of my classifier predictions.
1. Training the model
    * The model was trained using the LogisticRegression classifier algorithm from sklearn.
1. Testing the model
    * The model was tested for accuracy using the sklearn score function.

**What are the benefits of the parameter sampler you chose?**
I used RandomParameterSampling because this dataset was new to me and a benefit of this sampler is that its great for discovery and to help me to refine my search space if I want to continue to get try and improve the model.

**What are the benefits of the early stopping policy you chose?**
I used the BaditPolicy which uses a slack factor and an evaluation interval to determine when to terminate a run early because it allows me to set baseline number evalutaions then stop later runs early so as not to waste time or compute.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
More data
I would try using Baeysian Sampling instead of Random Parameter Sampling because bayesian sampling takes into account previous to have a better chance at getting better hyperparameters. I still may have started with random sampling to get an general idea of what I'd want to use in my Bayesian Sampling.
AutoMl more time or it train and optimal model 
## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
