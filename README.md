# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains bank marketing data about many different people and whether they have agreed to sign up for what ever is being marketed. We seek to predict based will say yes or no to what they are being marketed.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best perfoming model was the VotingEnsemble model genrated by AutoML with an accuracy of 0.9163 which was slightly better than the Scikit-learn pipeline which produced a model with an accuracy of 0.8971. This is likely because automl was able to select from many different models to find a more optimal solution for prediction.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline architechture (run notebook [here](./hyperdrive_model/udacity-project-hyperdrive.ipynb)):

1. Get the data
    * We get the data from in csv format from the following [link](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv)
1. Cleaning the data
    * The data is cleaned using a python function which was provided in the train.py script
    * Most of the clean up consists of taking binary values and replacing them with 0 or 1
1. Splitting the data
    * The data is split into traing and test sets using the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn. I decided to use split the data 80 - 20 meaning 80% of the data is ussed for training and 20% is used for testing.
1. Hyperparameter Tuning with HyperDrive
    * For hyperparameter tuning I used hyperdrive to train the C parameter (Inverse of regularization strength) and the max_iter parameter (Maximum number of iterations to converge) I used Random Parameter Sampling for my parameter sampler (more info on that below) and a BanditPolicy for my early termination policy (more info below). I set up hyperdrive to maximize the accuracy of my classifier predictions.
1. Training the model
    * The model was trained using the LogisticRegression classifier algorithm from sklearn.
1. Testing the model
    * The model was tested for accuracy using the sklearn score function.

**What are the benefits of the parameter sampler you chose?**
I used RandomParameterSampling because this dataset was new to me and a benefit of this sampler is that its great for discovery and to help me to refine my search space if I want to continue to get try and improve the model.

**What are the benefits of the early stopping policy you chose?**
I used the BaditPolicy which uses a slack factor and an evaluation interval to determine when to terminate a run early because it allows me to set baseline number evalutaions then stop later runs early so as not to waste time or compute.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
Below is the output from AutoML and the different classifiers it tried. The best model it found was the VotingEnsemble model, which takes the average of multiple regression models to find the best prediction. AutoML run info [here](./automl/udacity-project-automl.ipynb)
```
 ITER   PIPELINE                                       DURATION            METRIC      BEST
    0   MaxAbsScaler LightGBM                          0:00:11             0.9129    0.9129
    1   MaxAbsScaler XGBoostClassifier                 0:00:17             0.9138    0.9138
    2   MaxAbsScaler ExtremeRandomTrees                0:00:16             0.7288    0.9138
    3   SparseNormalizer XGBoostClassifier             0:00:13             0.9119    0.9138
    4   MaxAbsScaler LightGBM                          0:00:09             0.9099    0.9138
    5   MaxAbsScaler LightGBM                          0:00:09             0.8872    0.9138
    6   StandardScalerWrapper XGBoostClassifier        0:00:12             0.9076    0.9138
    7   MaxAbsScaler LogisticRegression                0:00:14             0.9066    0.9138
    8   StandardScalerWrapper ExtremeRandomTrees       0:00:10             0.8871    0.9138
    9   StandardScalerWrapper XGBoostClassifier        0:00:11             0.9083    0.9138
   10   SparseNormalizer LightGBM                      0:00:09             0.9031    0.9138
   11   StandardScalerWrapper XGBoostClassifier        0:00:10             0.9104    0.9138
   12   MaxAbsScaler LogisticRegression                0:00:14             0.9064    0.9138
   13   MaxAbsScaler SGD                               0:00:09             0.8628    0.9138
   14   StandardScalerWrapper XGBoostClassifier        0:00:12             0.9118    0.9138
   15   SparseNormalizer RandomForest                  0:00:26             0.8173    0.9138
   16   StandardScalerWrapper LogisticRegression       0:00:11             0.9071    0.9138
   17   StandardScalerWrapper RandomForest             0:00:15             0.8992    0.9138
   18   StandardScalerWrapper XGBoostClassifier        0:00:15             0.9122    0.9138
   19   TruncatedSVDWrapper RandomForest               0:02:28             0.8194    0.9138
   20   TruncatedSVDWrapper RandomForest               0:01:26             0.8323    0.9138
   21   StandardScalerWrapper XGBoostClassifier        0:00:04             0.9118    0.9138
   22   StandardScalerWrapper LightGBM                 0:00:04             0.9063    0.9138
   23                                                  0:10:01                nan    0.9138
ERROR: {
    "additional_properties": {},
    "error": {
        "additional_properties": {
            "debugInfo": null
        },
        "code": "ServiceError",
        "severity": null,
        "message": "Execution Service failed to start the working state for the run. Please try again.",
        "message_format": null,
        "message_parameters": null,
        "reference_code": null,
        "details_uri": null,
        "target": null,
        "details": [],
        "inner_error": null,
        "additional_info": null
    },
    "correlation": {
        "operation": "c2e7e46985c2ffc93e668badd53569ff",
        "request": "72d8298e0b431148"
    },
    "environment": "southcentralus",
    "location": "southcentralus",
    "time": {},
    "component_name": "execution-worker"
}
   24   SparseNormalizer XGBoostClassifier             0:00:04             0.9118    0.9138
   25   SparseNormalizer XGBoostClassifier             0:00:04             0.9119    0.9138
   26   MaxAbsScaler LogisticRegression                0:00:04             0.9064    0.9138
   27   MaxAbsScaler LogisticRegression                0:00:04             0.9068    0.9138
   28   SparseNormalizer XGBoostClassifier             0:00:04             0.9115    0.9138
   29   TruncatedSVDWrapper XGBoostClassifier          0:00:04             0.9108    0.9138
   30    VotingEnsemble                                0:00:04             0.9163    0.9163
```

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The Scikit pipeline had a lower accuracy but I think this was mostly due to the fact that it was only using one type of model so even with finding good hyperparameters with hyperdrive there was always a chance automl would be able to try a model that was more suited for the data set. The automl architecture was simpler as all that was required was to get the data into the correct format and let automl take it from there. Meanwhile, for the scikit pipeline on top of getting the data I had to find the best hyperparameters for the model.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
I think there a few areas in which I could improve the results from this experiment:
1. Get more data good data and balance out the dataset the given dataset wasn't very balanaced as there were many more no outcomes than yes.
1. I could try Bayesian Sampling after doing Random Sampling to get and idea of the range of the best hyperparameters to really nail down the best options for hyperparameters in the sciki learn pipeline.
1. For AutoMl alotting more time for it try different models may help to find a better model.
1. I could also combine the approaches by first using AutoML to find the best options for model pipelines then use hyperdrive to find the best hyperparameters for those options
